{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e7c55dc",
   "metadata": {},
   "source": [
    "# Assignment 04 – Reasoning\n",
    "In this assignment you will explore **reasoning** in large language models, experiment with *chain‑of‑thought* prompting, and reflect on two recent position papers debating whether LLMs truly “think.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1&nbsp; What is Reasoning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f47e6bd",
   "metadata": {},
   "source": [
    "Write **your own definition** of reasoning in the context of intelligent systems.  \n",
    "*Hints:* consider notions such as logical inference, causal deduction, abstraction, multi‑step planning, and how humans articulate intermediate thoughts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2&nbsp; Build a Basic Chain‑of‑Thought (CoT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb5f359",
   "metadata": {},
   "source": [
    "### 2 · Build a Basic Chain‑of‑Thought (CoT)\n",
    "\n",
    "Your goal is to wrap **any LLM backend of your choice** with a helper that can optionally trigger a *chain‑of‑thought* style response.\n",
    "\n",
    "#### What to implement\n",
    "\n",
    "1. **Choose a backend** (set `USE_BACKEND`):  \n",
    "   * `\"gemma\"` – use the `google/gemma-3-4b-it` checkpoint via 🤗 Transformers.  \n",
    "   * `\"openai\"` – route to your `call_openai()` helper (e.g., GPT‑4o).  \n",
    "   * `\"gemini\"` – route to your `call_gemini()` helper (e.g., Gemini 1.5 Pro).\n",
    "\n",
    "2. **Load / authenticate**  \n",
    "   * HF backends need `HF_TOKEN`.  \n",
    "   * OpenAI backends need `OPENAI_API_KEY`.  \n",
    "   * Gemini backends need `GOOGLE_API_KEY` + `GOOGLE_CSE_ID` or equivalent.\n",
    "\n",
    "3. **Implement `run_llm(prompt, with_cot=False)`**  \n",
    "   * When `with_cot=True`, prepend a CoT trigger such as **“Let’s think step by step.”**  \n",
    "   * Return the *model’s final answer* (you may choose to strip the intermediate thoughts).\n",
    "\n",
    "4. **Quick sanity‑check**  \n",
    "   * Call the helper once *without* and once *with* CoT on a simple prompt (e.g., *“Is 17 a prime number?”*) and print both outputs.\n",
    "\n",
    "**Reference:** Jason Wei, Xuezhi Wang, Dale Schuurmans, et al.  \n",
    "*“Chain‑of‑Thought Prompting Elicits Reasoning in Large Language Models.”*  \n",
    "arXiv:2201.11903 (2022) <https://arxiv.org/abs/2201.11903>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def call_openai(prompt: str, model_name: str = \"gpt-4o\") -> str:\n",
    "    \"\"\"Call OpenAI API and return generated text.\"\"\"\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def call_gemini(prompt: str, model_name: str = \"models/gemini-1.5-pro\") -> str:\n",
    "    \"\"\"Call Google Gemini API and return generated text.\"\"\"\n",
    "    import google.generativeai as genai\n",
    "    genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "    model = genai.GenerativeModel(model_name)\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c80f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- STUDENT TODOs BELOW -----------------------------\n",
    "# 1️⃣  Choose your backend: 'gemma', 'openai', or 'gemini'\n",
    "USE_BACKEND = \"gemini\"  # <-- change me\n",
    "\n",
    "# 2️⃣  Load / configure the model for the chosen backend\n",
    "if USE_BACKEND == \"gemma\":\n",
    "    # Gemma 3‑4B instruction‑tuned via Hugging Face 🧩\n",
    "    from transformers import AutoTokenizer, Gemma3ForConditionalGeneration, pipeline\n",
    "\n",
    "    MODEL_ID = \"google/gemma-3-4b-it\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=True)\n",
    "    model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=\"auto\",\n",
    "        token=True,                # ← relies on HF_TOKEN env variable\n",
    "    )\n",
    "    generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "elif USE_BACKEND == \"openai\":\n",
    "    # OpenAI helper must be defined elsewhere in the notebook\n",
    "    # Example: from my_helpers import call_openai\n",
    "    pass  # TODO: nothing to load – just make sure call_openai() is available\n",
    "\n",
    "elif USE_BACKEND == \"gemini\":\n",
    "    # Gemini helper must be defined elsewhere in the notebook\n",
    "    # Example: from my_helpers import call_gemini\n",
    "    pass  # TODO: nothing to load – just make sure call_gemini() is available\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Unsupported backend selected!\")\n",
    "\n",
    "# 3️⃣  Implement the CoT helper\n",
    "def run_llm(prompt: str, *, with_cot: bool = False, max_new_tokens: int = 256):\n",
    "    \"\"\"Run the chosen backend, optionally triggering chain‑of‑thought.\"\"\"\n",
    "    cot_prefix = \"INSERT YOUR PROMPT HERE\\n\"\n",
    "    full_prompt = cot_prefix + prompt if with_cot else prompt\n",
    "\n",
    "    if USE_BACKEND == \"gemma\":\n",
    "        return generator(full_prompt, max_new_tokens=max_new_tokens)[0][\"generated_text\"]\n",
    "\n",
    "    if USE_BACKEND == \"openai\":\n",
    "        # TODO: ensure `call_openai()` exists\n",
    "        return call_openai(full_prompt)\n",
    "\n",
    "    if USE_BACKEND == \"gemini\":\n",
    "        # TODO: ensure `call_gemini()` exists\n",
    "        return call_gemini(full_prompt)\n",
    "\n",
    "    raise RuntimeError(\"No valid backend route found.\")\n",
    "\n",
    "# 4️⃣  Sanity‑check – compare baseline vs. CoT\n",
    "_test_prompt = \"Is 17 a prime number?\"\n",
    "print(\"→ Baseline:\", run_llm(_test_prompt, with_cot=False))\n",
    "print(\"→ With CoT:\", run_llm(_test_prompt, with_cot=True))\n",
    "# -------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3&nbsp; Evaluate CoT Performance vs. Single‑Call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ed33e0",
   "metadata": {},
   "source": [
    "\n",
    "Pick **10 reasoning‑intensive questions** (e.g. a logic puzzle, word problem, or multi‑step arithmetic query).  \n",
    "Run the model twice: once *without* chain‑of‑thought and once *with* CoT using `run_llm`.  \n",
    "Manually (or programmatically) judge which output is *more correct, complete, and transparent*.  \n",
    "Record your observations below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aac7ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ▶️ Comparison template\n",
    "reasoning_question = \"If all bloops are meems and some meems are glorps, are all bloops definitely glorps? Explain your answer.\"\n",
    "\n",
    "baseline = run_llm(reasoning_question, with_cot=False)\n",
    "cot = run_llm(reasoning_question, with_cot=True)\n",
    "\n",
    "print(\"\\n--- Baseline ---\\n\", baseline)\n",
    "print(\"\\n--- CoT ---\\n\", cot)\n",
    "\n",
    "# TODO: Add your evaluation notes (e.g. accuracy, clarity) in the markdown cell that follows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4&nbsp; Read Two Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3c16ac",
   "metadata": {},
   "source": [
    "\n",
    "* **“The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity”** (Shojaee *et al.*, 2025).  \n",
    "  *ArXiv:* <https://arxiv.org/abs/2506.06941>\n",
    "* **“The Illusion of the Illusion of Thinking”** (Opus & Lawson, 2025) – a critical response.  \n",
    "  *ArXiv:* <https://arxiv.org/abs/2506.09250>\n",
    "\n",
    "Skim both (abstract → methods → main results) and take note of their competing claims about LRMs and chain‑of‑thought reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5&nbsp; Reflection (2 paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02422c52",
   "metadata": {},
   "source": [
    "\n",
    "In **≈150–250 words**, explain **which paper’s argument you find more convincing and why**.  \n",
    "Consider the authors’ experimental setups, evidence, interpretation of “reasoning,” and any limitations you notice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Section 6 — Implementing Self‑Consistency Decoding\n",
    "\n",
    "**Learning goal.** Experience how self‑consistency improves reasoning accuracy by sampling diverse chain‑of‑thoughts and aggregating answers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Single‑Model Self‑Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detailed Steps\n",
    "\n",
    "Follow the eight steps below to implement self‑consistency with a *single* language model.\n",
    "\n",
    "| Step | What you do | Why it matters |\n",
    "|------|-------------|----------------|\n",
    "| **1. Choose a base model** | Pick **one** of the foundation models you have tried earlier (e.g., GPT‑4o, Claude‑3 Sonnet, Gemini 1.5 Pro, etc.). Set it in the `MODEL` constant below. | Keeps the experiment focused and cost‑controlled. |\n",
    "| **2. Select tasks** | Re‑use at least **five reasoning problems** from previous sections and store them in `TASKS`. Each task must include a ground‑truth answer for evaluation. | Ensures comparability across sampling budgets. |\n",
    "| **3. Generate reasoning paths** | Write `generate_paths(question, n_paths)` that samples `n_paths` Chain‑of‑Thought explanations from the chosen model (temperature ≥ 0.7). | Diversity of reasoning paths is the heart of self‑consistency. |\n",
    "| **4. Parse final answers** | Implement `extract_final_answer(full_response)` that returns the answer string from a model response. | Needed for voting. |\n",
    "| **5. Majority vote aggregator** | Implement `majority_vote(answers)` that returns the most frequent answer and its support size. Break ties by picking the answer whose chain has the highest average log‑probability. | Converts diverse chains into a single prediction. |\n",
    "| **6. Run experiments** | For each sampling budget **k ∈ {3, 5, 10}** and each task, generate `k` paths → vote → record whether the voted answer matches ground truth. | Measures accuracy vs. compute. |\n",
    "| **7. Collect metrics** | Track (a) accuracy, (b) average latency, (c) total tokens. Store them in a `pandas.DataFrame`. | Enables quantitative comparison. |\n",
    "| **8. Analyze results** | Plot / tabulate metrics and write a short discussion: *Where do returns diminish? How does cost scale?* | Connects empirical findings to theory. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀 Imports & configuration\n",
    "import time, json, collections, statistics\n",
    "import pandas as pd\n",
    "\n",
    "# TODO: 🔑 Add your API key if needed, e.g. openai.api_key = \"sk-...\"\n",
    "# import openai\n",
    "\n",
    "MODEL = \"gpt-4o-mini\"  # TODO: replace with your chosen model\n",
    "TEMPERATURE = 0.7      # Non‑zero for diverse sampling\n",
    "MAX_TOKENS = 1200\n",
    "\n",
    "# 👇 Populate with at least 5 {question, answer} dicts\n",
    "TASKS = [\n",
    "    # {\"question\": \"What is 13 * 7?\", \"answer\": \"91\"},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_paths(question: str, n_paths: int):\n",
    "    \"\"\"Return a list of full model responses (thought + answer).\"\"\"\n",
    "    paths = []\n",
    "    for _ in range(n_paths):\n",
    "        # TODO: 🔄 Call your LLM here with Chain‑of‑Thought prompting\n",
    "        # Example pseudo‑call (replace with the correct SDK/method):\n",
    "        # response = openai.ChatCompletion.create(\n",
    "        #     model=MODEL,\n",
    "        #     messages=[\n",
    "        #         {\"role\": \"user\", \"content\": f\"{question}\\n\\nLet's think step by step.\"}\n",
    "        #     ],\n",
    "        #     temperature=TEMPERATURE,\n",
    "        #     max_tokens=MAX_TOKENS,\n",
    "        # )\n",
    "        # paths.append(response['choices'][0]['message']['content'])\n",
    "        pass\n",
    "    return paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_answer(response: str) -> str:\n",
    "    \"\"\"Extract the answer after the last occurrence of 'Answer:' (simple heuristic).\"\"\"\n",
    "    import re\n",
    "    match = re.findall(r\"Answer\\s*[:=]\\s*(.*)\", response, flags=re.IGNORECASE)\n",
    "    return match[-1].strip() if match else \"\"\n",
    "\n",
    "def majority_vote(answers):\n",
    "    \"\"\"Return (winning_answer, support_count, counts_dict).\"\"\"\n",
    "    counts = collections.Counter(answers)\n",
    "    winner, support = counts.most_common(1)[0]\n",
    "    return winner, support, counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "records = []\n",
    "\n",
    "for k in [3, 5, 10]:\n",
    "    for task in TASKS:\n",
    "        question, truth = task['question'], task['answer']\n",
    "        t0 = time.time()\n",
    "        paths = generate_paths(question, k)\n",
    "        gen_time = time.time() - t0\n",
    "\n",
    "        answers = [extract_final_answer(p) for p in paths]\n",
    "        pred, support, _ = majority_vote(answers)\n",
    "        is_correct = (pred == truth)\n",
    "\n",
    "        records.append({\n",
    "            'k_paths': k,\n",
    "            'question': question,\n",
    "            'truth': truth,\n",
    "            'predicted': pred,\n",
    "            'support': support,\n",
    "            'latency_sec': round(gen_time, 2),\n",
    "            'correct': is_correct,\n",
    "            # TODO: add token_usage if available from your SDK\n",
    "        })\n",
    "\n",
    "df_results = pd.DataFrame(records)\n",
    "df_results.groupby('k_paths')['correct'].mean().rename('accuracy').to_frame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Inspect detailed results\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✍️ Reflection\n",
    "# In a new markdown cell below, discuss:\n",
    "# - How accuracy changes with k\n",
    "# - Cost/latency implications\n",
    "# - Any qualitative observations about reasoning diversity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Cross‑Model Self‑Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detailed Steps\n",
    "\n",
    "This experiment ensembles **five distinct language models** by majority voting over *one* reasoning path from each model.\n",
    "\n",
    "| Step | What you do | Why it matters |\n",
    "|------|-------------|----------------|\n",
    "| **1. Pick five models** | Populate the `MODELS` list below with at least five different LLM identifiers available to you (e.g., `gpt‑4o`, `claude‑3‑opus`, `gemini‑1.5‑pro‑latest`, `llama‑3‑70b‑instruct`, `mistral‑large`). | Horizontal diversity often yields complementary reasoning. |\n",
    "| **2. Re‑use the tasks** | Use the same `TASKS` list created in Section 6.1 so results are comparable. | Controls for task variation. |\n",
    "| **3. Generate one path per model** | Implement `generate_one_path(question, model)` that returns a single Chain‑of‑Thought response from the given model (*temperature ≈ 0* for deterministic decoding). | Mimics a cost‑constrained ensemble where each model fires once. |\n",
    "| **4. Parse answers** | Re‑use `extract_final_answer` from 6.1 to extract each model’s answer string. | Enables voting. |\n",
    "| **5. Majority vote across models** | Fill in `majority_vote(answers)` (already defined) to aggregate answers across models and return the winning answer + support. | Core of ensemble self‑consistency. |\n",
    "| **6. Run the experiment** | For every task, call each model → vote → record accuracy, per‑task support distribution, latency, and token cost. | Produces cross‑model performance metrics. |\n",
    "| **7. Compare strategies** | Tabulate accuracy/latency/cost vs. the 10‑path single‑model result from Section 6.1. | Quantifies trade‑offs between vertical and horizontal ensembles. |\n",
    "| **8. Reflect** | Discuss which method you’d choose under (a) limited budget, (b) need for highest accuracy, and why. | Connects empirical evidence to deployment choices. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀 Configuration for cross‑model ensemble\n",
    "import time, collections, statistics\n",
    "import pandas as pd\n",
    "\n",
    "MODELS = [\n",
    "    # \"gpt-4o-mini\",\n",
    "    # \"claude-3-haiku\",\n",
    "    # \"gemini-1.5-pro\",\n",
    "    # \"llama-3-70b-instruct\",\n",
    "    # \"mistral-large\",\n",
    "]\n",
    "TEMPERATURE_CROSS = 0.0  # Near-deterministic decoding\n",
    "MAX_TOKENS = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_one_path(question: str, model: str) -> str:\n",
    "    \"\"\"Return a Chain‑of‑Thought response from *one* model run.\"\"\"\n",
    "    # TODO: 🔄 Replace the pseudo‑call with your provider's SDK.\n",
    "    # Example:\n",
    "    # response = openai.ChatCompletion.create(\n",
    "    #     model=model,\n",
    "    #     messages=[\n",
    "    #         {\"role\": \"user\", \"content\": f\"{question}\\n\\nLet's think step by step.\"}\n",
    "    #     ],\n",
    "    #     temperature=TEMPERATURE_CROSS,\n",
    "    #     max_tokens=MAX_TOKENS,\n",
    "    # )\n",
    "    # return response['choices'][0]['message']['content']\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "records_cm = []\n",
    "\n",
    "for task in TASKS:\n",
    "    question, truth = task['question'], task['answer']\n",
    "    answers_by_model = {}\n",
    "    t0 = time.time()\n",
    "    for model in MODELS:\n",
    "        resp = generate_one_path(question, model)\n",
    "        ans = extract_final_answer(resp)\n",
    "        answers_by_model[model] = ans\n",
    "    latency = time.time() - t0\n",
    "\n",
    "    voted_answer, support, support_dict = majority_vote(list(answers_by_model.values()))\n",
    "    is_correct = (voted_answer == truth)\n",
    "\n",
    "    records_cm.append({\n",
    "        'question': question,\n",
    "        'truth': truth,\n",
    "        'predicted': voted_answer,\n",
    "        'support': support,\n",
    "        'support_breakdown': support_dict,\n",
    "        'latency_sec': round(latency, 2),\n",
    "        'correct': is_correct,\n",
    "        # TODO: aggregate token usage if your SDK provides it\n",
    "    })\n",
    "\n",
    "df_cm = pd.DataFrame(records_cm)\n",
    "df_cm['correct'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Cross‑model ensemble results\n",
    "df_cm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✍️ Reflection\n",
    "# In a new markdown cell below, compare:\n",
    "# - Accuracy vs. Section 6.1 (k=10)\n",
    "# - Latency & token costs\n",
    "# - Qualitative differences in reasoning styles across models\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
