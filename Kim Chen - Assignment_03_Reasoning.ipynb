{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e7c55dc",
   "metadata": {},
   "source": [
    "# Assignment 04 – Reasoning\n",
    "In this assignment you will explore **reasoning** in large language models, experiment with *chain‑of‑thought* prompting, and reflect on two recent position papers debating whether LLMs truly “think.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1&nbsp; What is Reasoning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f47e6bd",
   "metadata": {},
   "source": [
    "Write **your own definition** of reasoning in the context of intelligent systems.  \n",
    "*Hints:* consider notions such as logical inference, causal deduction, abstraction, multi‑step planning, and how humans articulate intermediate thoughts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2&nbsp; Build a Basic Chain‑of‑Thought (CoT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb5f359",
   "metadata": {},
   "source": [
    "### 2 · Build a Basic Chain‑of‑Thought (CoT)\n",
    "\n",
    "Your goal is to wrap **any LLM backend of your choice** with a helper that can optionally trigger a *chain‑of‑thought* style response.\n",
    "\n",
    "#### What to implement\n",
    "\n",
    "1. **Choose a backend** (set `USE_BACKEND`):  \n",
    "   * `\"gemma\"` – use the `google/gemma-3-4b-it` checkpoint via 🤗 Transformers.  \n",
    "   * `\"openai\"` – route to your `call_openai()` helper (e.g., GPT‑4o).  \n",
    "   * `\"gemini\"` – route to your `call_gemini()` helper (e.g., Gemini 1.5 Pro).\n",
    "\n",
    "2. **Load / authenticate**  \n",
    "   * HF backends need `HF_TOKEN`.  \n",
    "   * OpenAI backends need `OPENAI_API_KEY`.  \n",
    "   * Gemini backends need `GOOGLE_API_KEY` + `GOOGLE_CSE_ID` or equivalent.\n",
    "\n",
    "3. **Implement `run_llm(prompt, with_cot=False)`**  \n",
    "   * When `with_cot=True`, prepend a CoT trigger such as **“Let’s think step by step.”**  \n",
    "   * Return the *model’s final answer* (you may choose to strip the intermediate thoughts).\n",
    "\n",
    "4. **Quick sanity‑check**  \n",
    "   * Call the helper once *without* and once *with* CoT on a simple prompt (e.g., *“Is 17 a prime number?”*) and print both outputs.\n",
    "\n",
    "**Reference:** Jason Wei, Xuezhi Wang, Dale Schuurmans, et al.  \n",
    "*“Chain‑of‑Thought Prompting Elicits Reasoning in Large Language Models.”*  \n",
    "arXiv:2201.11903 (2022) <https://arxiv.org/abs/2201.11903>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access variables\n",
    "gemini_api_key = os.getenv(\"gemini_api_key\")\n",
    "openai_api_key = os.getenv(\"openai_api_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def call_openai(prompt: str, model_name: str = \"gpt-4o\") -> str:\n",
    "    \"\"\"Call OpenAI API and return generated text.\"\"\"\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(api_key=os.environ.get(\"openai_api_key\"))\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "import os, time\n",
    "from google.api_core import exceptions as gexc\n",
    "\n",
    "def call_gemini(\n",
    "    prompt: str,\n",
    "    model_name: str = \"models/gemini-1.5-pro\",\n",
    "    temperature: float = None,\n",
    "    max_output_tokens: int = None,\n",
    "    retries: int = 3,\n",
    "    timeout_sec: float = 60.0,\n",
    ") -> str:\n",
    "    \"\"\"Call Gemini with timeout/retries; trims token budget on retries.\"\"\"\n",
    "    import google.generativeai as genai\n",
    "\n",
    "    api_key = os.environ.get(\"GEMINI_API_KEY\") or os.environ.get(\"gemini_api_key\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"Missing GEMINI_API_KEY in environment.\")\n",
    "\n",
    "    genai.configure(api_key=api_key)\n",
    "    model = genai.GenerativeModel(model_name)\n",
    "\n",
    "    # Use your globals if not provided\n",
    "    t = TEMPERATURE if temperature is None else temperature\n",
    "    max_tok = MAX_TOKENS if max_output_tokens is None else max_output_tokens\n",
    "\n",
    "    backoff = 1.0\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            resp = model.generate_content(\n",
    "                prompt,\n",
    "                generation_config={\n",
    "                    \"temperature\": float(t),\n",
    "                    # clamp to avoid very long generations that tend to 504\n",
    "                    \"max_output_tokens\": int(max(256, min(max_tok, 1024))),\n",
    "                },\n",
    "                request_options={\"timeout\": timeout_sec},\n",
    "            )\n",
    "            text = (getattr(resp, \"text\", \"\") or \"\").strip()\n",
    "            if text:\n",
    "                return text\n",
    "            # Fallback: stitch candidate parts if .text is empty\n",
    "            if getattr(resp, \"candidates\", None):\n",
    "                parts = []\n",
    "                for cand in resp.candidates:\n",
    "                    for part in getattr(getattr(cand, \"content\", None), \"parts\", []) or []:\n",
    "                        if hasattr(part, \"text\") and part.text:\n",
    "                            parts.append(part.text)\n",
    "                return \"\\n\".join(parts).strip()\n",
    "            return \"\"\n",
    "        except (gexc.DeadlineExceeded, gexc.ResourceExhausted, gexc.ServiceUnavailable, gexc.InternalServerError) as e:\n",
    "            if attempt == retries - 1:\n",
    "                raise\n",
    "            # brief backoff + reduce budget and try again\n",
    "            time.sleep(backoff + 0.15 * attempt)\n",
    "            max_tok = max(256, int(max_tok * 0.7))\n",
    "            backoff *= 2.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c80f743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Baseline: Yes, 17 is a prime number.  It's only divisible by 1 and itself.\n",
      "→ With CoT: Yes, 17 is a prime number.  It is only divisible by 1 and itself.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------- STUDENT TODOs BELOW -----------------------------\n",
    "# 1️⃣  Choose your backend: 'gemma', 'openai', or 'gemini'\n",
    "USE_BACKEND = \"gemini\"  # <-- change me\n",
    "\n",
    "# 2️⃣  Load / configure the model for the chosen backend\n",
    "if USE_BACKEND == \"gemma\":\n",
    "    # Gemma 3‑4B instruction‑tuned via Hugging Face 🧩\n",
    "    from transformers import AutoTokenizer, Gemma3ForConditionalGeneration, pipeline\n",
    "\n",
    "    MODEL_ID = \"google/gemma-3-4b-it\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=True)\n",
    "    model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=\"auto\",\n",
    "        token=True,                # ← relies on HF_TOKEN env variable\n",
    "    )\n",
    "    generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "elif USE_BACKEND == \"openai\":\n",
    "    # OpenAI helper must be defined elsewhere in the notebook\n",
    "    # Example: from my_helpers import call_openai\n",
    "    pass  # TODO: nothing to load – just make sure call_openai() is available\n",
    "\n",
    "elif USE_BACKEND == \"gemini\":\n",
    "    # Gemini helper must be defined elsewhere in the notebook\n",
    "    # Example: from my_helpers import call_gemini\n",
    "    pass  # TODO: nothing to load – just make sure call_gemini() is available\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Unsupported backend selected!\")\n",
    "\n",
    "# 3️⃣  Implement the CoT helper\n",
    "def run_llm(prompt: str, *, with_cot: bool = False, max_new_tokens: int = 256):\n",
    "    \"\"\"Run the chosen backend, optionally triggering chain‑of‑thought.\"\"\"\n",
    "    cot_prefix = \"INSERT YOUR PROMPT HERE\\n\"\n",
    "    full_prompt = cot_prefix + prompt if with_cot else prompt\n",
    "\n",
    "    if USE_BACKEND == \"gemma\":\n",
    "        return generator(full_prompt, max_new_tokens=max_new_tokens)[0][\"generated_text\"]\n",
    "\n",
    "    if USE_BACKEND == \"openai\":\n",
    "        # TODO: ensure `call_openai()` exists\n",
    "        return call_openai(full_prompt)\n",
    "\n",
    "    if USE_BACKEND == \"gemini\":\n",
    "        # TODO: ensure `call_gemini()` exists\n",
    "        return call_gemini(full_prompt)\n",
    "\n",
    "    raise RuntimeError(\"No valid backend route found.\")\n",
    "\n",
    "# 4️⃣  Sanity‑check – compare baseline vs. CoT\n",
    "_test_prompt = \"Is 17 a prime number?\"\n",
    "print(\"→ Baseline:\", run_llm(_test_prompt, with_cot=False))\n",
    "print(\"→ With CoT:\", run_llm(_test_prompt, with_cot=True))\n",
    "# -------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3&nbsp; Evaluate CoT Performance vs. Single‑Call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ed33e0",
   "metadata": {},
   "source": [
    "\n",
    "Pick **10 reasoning‑intensive questions** (e.g. a logic puzzle, word problem, or multi‑step arithmetic query).  \n",
    "Run the model twice: once *without* chain‑of‑thought and once *with* CoT using `run_llm`.  \n",
    "Manually (or programmatically) judge which output is *more correct, complete, and transparent*.  \n",
    "Record your observations below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9aac7ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Baseline ---\n",
      " No, all bloops are definitely *not* glorps.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "Imagine meems as a big circle.  Inside that circle is a smaller circle representing all bloops.  Now, glorps are represented by another circle that *overlaps* with the meems circle, but it doesn't necessarily encompass the entire bloops circle.  Some meems are glorps, but not all of them have to be.  Since bloops are entirely within the meems circle, they could fall within the overlapping section with glorps, or they could be in a part of the meems circle that doesn't overlap at all.\n",
      "\n",
      "Therefore, some bloops *might* be glorps, but it's not guaranteed that all of them are.\n",
      "\n",
      "--- CoT ---\n",
      " No, all bloops are not definitely glorps.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* **All bloops are meems:**  This means the set of bloops is entirely contained within the set of meems.\n",
      "* **Some meems are glorps:** This means there's an overlap between the set of meems and the set of glorps, but it doesn't tell us anything about how that overlap relates to the set of bloops.  The overlapping part of meems and glorps might include some or none of the bloops.\n",
      "\n",
      "It's possible that some or even all bloops *could* be glorps, but it's not guaranteed.  We only know they are all meems, and meems *sometimes* are glorps.\n",
      "\n",
      "\n",
      "Think of it with a real-world example:\n",
      "\n",
      "* All dogs are mammals.\n",
      "* Some mammals are cats.\n",
      "\n",
      "Does this mean all dogs are cats?  Clearly not.  Dogs and cats are both mammals, but entirely separate groups.  The same logic applies to bloops, meems, and glorps.\n"
     ]
    }
   ],
   "source": [
    "# ▶️ Comparison template\n",
    "reasoning_question = \"If all bloops are meems and some meems are glorps, are all bloops definitely glorps? Explain your answer.\"\n",
    "\n",
    "baseline = run_llm(reasoning_question, with_cot=False)\n",
    "cot = run_llm(reasoning_question, with_cot=True)\n",
    "\n",
    "print(\"\\n--- Baseline ---\\n\", baseline)\n",
    "print(\"\\n--- CoT ---\\n\", cot)\n",
    "\n",
    "# TODO: Add your evaluation notes (e.g. accuracy, clarity) in the markdown cell that follows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: Both Baseline and CoT give the correct conclusion (“not necessarily / no”).\n",
    "Completeness: Baseline provides a clear Venn-diagram intuition; CoT adds explicit set-containment statements and a concrete real-world analogy (dogs/cats), covering edge cases (some or none of the bloops in the overlap).\n",
    "Transparency (step-by-step): CoT is more structured: it enumerates premises, explains what each implies, and contrasts possible configurations; Baseline explains well but with fewer explicit intermediate steps.\n",
    "Clarity: Both are readable; CoT’s bullets and analogy make it easier to follow for a non-expert.\n",
    "Verdict: CoT wins (more complete and more transparent while equally accurate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4&nbsp; Read Two Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3c16ac",
   "metadata": {},
   "source": [
    "\n",
    "* **“The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity”** (Shojaee *et al.*, 2025).  \n",
    "  *ArXiv:* <https://arxiv.org/abs/2506.06941>\n",
    "* **“The Illusion of the Illusion of Thinking”** (Opus & Lawson, 2025) – a critical response.  \n",
    "  *ArXiv:* <https://arxiv.org/abs/2506.09250>\n",
    "\n",
    "Skim both (abstract → methods → main results) and take note of their competing claims about LRMs and chain‑of‑thought reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shojaee et al. (2025) — The Illusion of Thinking\n",
    "\n",
    "Abstract:\n",
    "\n",
    "Argues that while CoT-enabled LRMs appear to “think,” this ability collapses as problem complexity increases. CoT gives temporary benefits on medium tasks, but fundamentally fails on harder ones.\n",
    "\n",
    "Methods:\n",
    "\n",
    "1.Built controlled benchmark puzzles (Tower of Hanoi, River Crossing variants) where problem complexity could be scaled in a principled way.\n",
    "2.Measured both final accuracy and reasoning effort (tokens used, intermediate steps).\n",
    "\n",
    "Main Results:\n",
    "\n",
    "Identified three regimes:\n",
    "1.Low complexity: Plain LLMs (no CoT) outperform.\n",
    "2.Medium complexity: LRMs with CoT perform better.\n",
    "3.High complexity: Both collapse completely — accuracy falls to ~0.\n",
    "\n",
    "Found a “token-effort curve”: models invest more reasoning tokens as difficulty rises, then unexpectedly cut effort at the hardest levels — not due to token budget limits, but intrinsic limits.\n",
    "\n",
    "Concluded CoT is not true reasoning, but an illusion that fails under real complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opus & Lawson (2025) — The Illusion of the Illusion of Thinking\n",
    "\n",
    "Abstract:\n",
    "Critiques Shojaee et al., claiming the “collapse” they report is an artifact of experimental design rather than a fundamental failure of reasoning.\n",
    "\n",
    "Methods:\n",
    "\n",
    "Re-examined Shojaee’s datasets and evaluation pipeline.\n",
    "\n",
    "Identified two major flaws:\n",
    "\n",
    "Token/output artifacts: In Tower of Hanoi, models often self-terminated outputs (“too long to finish”), which was mis-scored as reasoning failure.\n",
    "\n",
    "Unsolvable tasks: Some River Crossing puzzles were mathematically impossible, yet models that correctly flagged impossibility were scored as wrong.\n",
    "\n",
    "Proposed alternative evaluation: ask models for general rules/algorithms instead of exhaustive outputs.\n",
    "\n",
    "Main Results:\n",
    "\n",
    "When scored differently, models did not exhibit “collapse”; they could generate correct algorithmic solutions even on higher-complexity tasks.\n",
    "\n",
    "Concluded the observed failures were measurement artifacts.\n",
    "\n",
    "Argues LRMs with CoT do retain genuine reasoning abilities, though brittle in execution if forced into long step-by-step output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5&nbsp; Reflection (2 paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02422c52",
   "metadata": {},
   "source": [
    "\n",
    "In **≈150–250 words**, explain **which paper’s argument you find more convincing and why**.  \n",
    "Consider the authors’ experimental setups, evidence, interpretation of “reasoning,” and any limitations you notice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between the two, I find Opus & Lawson’s critique more convincing, though both papers highlight important insights. Shojaee et al. deserve credit for building carefully controlled puzzles and showing that LRMs with chain-of-thought do not scale smoothly with complexity. Their three-regime finding is striking, and the “effort curve” suggests genuine limits. However, their evaluation conflates reasoning ability with output format: requiring models to enumerate long sequences of moves or solve unsolvable tasks risks measuring verbosity or persistence rather than reasoning per se.\n",
    "\n",
    "Opus & Lawson persuasively argue that these design choices artificially produced the “collapse.” Their reanalysis shows that when models are asked to generate rules or algorithms—a more faithful test of reasoning—they perform well even on complex instances. This aligns more closely with how humans demonstrate understanding: by abstracting general solutions rather than exhaustively writing every step. Their paper reframes the debate, suggesting the illusion lies in our benchmarks, not necessarily in the models.\n",
    "\n",
    "That said, Opus & Lawson focus mainly on exposing flaws rather than proving that LRMs possess robust reasoning. Their results show models can generalize, but brittleness remains. Overall, I lean toward their interpretation: Shojaee et al.’s “collapse” seems more an artifact of task framing than evidence of fundamental incapacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Section 6 — Implementing Self‑Consistency Decoding\n",
    "\n",
    "**Learning goal.** Experience how self‑consistency improves reasoning accuracy by sampling diverse chain‑of‑thoughts and aggregating answers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Single‑Model Self‑Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detailed Steps\n",
    "\n",
    "Follow the eight steps below to implement self‑consistency with a *single* language model.\n",
    "\n",
    "| Step | What you do | Why it matters |\n",
    "|------|-------------|----------------|\n",
    "| **1. Choose a base model** | Pick **one** of the foundation models you have tried earlier (e.g., GPT‑4o, Claude‑3 Sonnet, Gemini 1.5 Pro, etc.). Set it in the `MODEL` constant below. | Keeps the experiment focused and cost‑controlled. |\n",
    "| **2. Select tasks** | Re‑use at least **five reasoning problems** from previous sections and store them in `TASKS`. Each task must include a ground‑truth answer for evaluation. | Ensures comparability across sampling budgets. |\n",
    "| **3. Generate reasoning paths** | Write `generate_paths(question, n_paths)` that samples `n_paths` Chain‑of‑Thought explanations from the chosen model (temperature ≥ 0.7). | Diversity of reasoning paths is the heart of self‑consistency. |\n",
    "| **4. Parse final answers** | Implement `extract_final_answer(full_response)` that returns the answer string from a model response. | Needed for voting. |\n",
    "| **5. Majority vote aggregator** | Implement `majority_vote(answers)` that returns the most frequent answer and its support size. Break ties by picking the answer whose chain has the highest average log‑probability. | Converts diverse chains into a single prediction. |\n",
    "| **6. Run experiments** | For each sampling budget **k ∈ {3, 5, 10}** and each task, generate `k` paths → vote → record whether the voted answer matches ground truth. | Measures accuracy vs. compute. |\n",
    "| **7. Collect metrics** | Track (a) accuracy, (b) average latency, (c) total tokens. Store them in a `pandas.DataFrame`. | Enables quantitative comparison. |\n",
    "| **8. Analyze results** | Plot / tabulate metrics and write a short discussion: *Where do returns diminish? How does cost scale?* | Connects empirical findings to theory. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀 Imports & configuration\n",
    "import time, json, collections, statistics\n",
    "import pandas as pd\n",
    "\n",
    "# TODO: 🔑 Add your API key if needed, e.g. openai.api_key = \"sk-...\"\n",
    "# import openai\n",
    "\n",
    "MODEL = \"models/gemini-1.5-pro\"  # ← correct format\n",
    "  # TODO: replace with your chosen model\n",
    "TEMPERATURE = 0.7      # Non‑zero for diverse sampling\n",
    "MAX_TOKENS = 1200\n",
    "\n",
    "# 👇 Populate with at least 5 {question, answer} dicts\n",
    "TASKS = [\n",
    "    {\n",
    "        \"question\": \"What is 13 * 7?\",\n",
    "        \"answer\": \"91\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"If a train travels 120 km in 2 hours, what is its average speed?\",\n",
    "        \"answer\": \"60 km/h\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Solve for x: 2(x − 3) + 4 = 10\",\n",
    "        \"answer\": \"x = 5\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"A bag has 4 red and 6 blue balls. What is the probability of drawing a red ball?\",\n",
    "        \"answer\": \"0.4\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"If all bloops are meems and some meems are glorps, are all bloops definitely glorps?\",\n",
    "        \"answer\": \"No, not necessarily\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"You drive 60 km at 60 km/h and return 60 km at 30 km/h. What is your average speed?\",\n",
    "        \"answer\": \"40 km/h\"\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUEST_PAUSE_SEC = 0.1  # tiny spacing to avoid bursts\n",
    "\n",
    "def generate_paths(question: str, n_paths: int):\n",
    "    \"\"\"Return a list of full model responses (thought + answer).\"\"\"\n",
    "    paths = []\n",
    "    for _ in range(n_paths):\n",
    "        prompt = (\n",
    "            f\"{question}\\n\\n\"\n",
    "            \"Think step by step in a few concise steps.\\n\"\n",
    "            \"Finish with a single line: Answer: <final answer>.\"\n",
    "        )\n",
    "        txt = call_gemini(\n",
    "            prompt,\n",
    "            model_name=MODEL,               # ensure MODEL == \"models/gemini-1.5-pro\"\n",
    "            temperature=TEMPERATURE,\n",
    "            max_output_tokens=MAX_TOKENS,   # e.g., start at 600–900 to reduce timeouts\n",
    "            retries=3,\n",
    "            timeout_sec=60.0,\n",
    "        )\n",
    "        paths.append(txt)\n",
    "        time.sleep(REQUEST_PAUSE_SEC)\n",
    "    return paths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_answer(response: str) -> str:\n",
    "    \"\"\"Extract the answer after the last occurrence of 'Answer:' (simple heuristic).\"\"\"\n",
    "    import re\n",
    "    match = re.findall(r\"Answer\\s*[:=]\\s*(.*)\", response, flags=re.IGNORECASE)\n",
    "    return match[-1].strip() if match else \"\"\n",
    "\n",
    "def majority_vote(answers):\n",
    "    \"\"\"Return (winning_answer, support_count, counts_dict).\"\"\"\n",
    "    counts = collections.Counter(answers)\n",
    "    winner, support = counts.most_common(1)[0]\n",
    "    return winner, support, counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.39 s, sys: 907 ms, total: 2.29 s\n",
      "Wall time: 6min 20s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k_paths</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         accuracy\n",
       "k_paths          \n",
       "3        0.166667\n",
       "5        0.166667\n",
       "10       0.166667"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "records = []\n",
    "\n",
    "for k in [3, 5, 10]:\n",
    "    for task in TASKS:\n",
    "        question, truth = task['question'], task['answer']\n",
    "        t0 = time.time()\n",
    "        paths = generate_paths(question, k)\n",
    "        gen_time = time.time() - t0\n",
    "\n",
    "        answers = [extract_final_answer(p) for p in paths]\n",
    "        pred, support, _ = majority_vote(answers)\n",
    "        is_correct = (pred == truth)\n",
    "\n",
    "        records.append({\n",
    "            'k_paths': k,\n",
    "            'question': question,\n",
    "            'truth': truth,\n",
    "            'predicted': pred,\n",
    "            'support': support,\n",
    "            'latency_sec': round(gen_time, 2),\n",
    "            'correct': is_correct,\n",
    "            # TODO: add token_usage if available from your SDK\n",
    "        })\n",
    "\n",
    "df_results = pd.DataFrame(records)\n",
    "df_results.groupby('k_paths')['correct'].mean().rename('accuracy').to_frame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k_paths</th>\n",
       "      <th>question</th>\n",
       "      <th>truth</th>\n",
       "      <th>predicted</th>\n",
       "      <th>support</th>\n",
       "      <th>latency_sec</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>What is 13 * 7?</td>\n",
       "      <td>91</td>\n",
       "      <td>91</td>\n",
       "      <td>3</td>\n",
       "      <td>4.91</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>If a train travels 120 km in 2 hours, what is ...</td>\n",
       "      <td>60 km/h</td>\n",
       "      <td>60 km/hour</td>\n",
       "      <td>3</td>\n",
       "      <td>4.57</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Solve for x: 2(x − 3) + 4 = 10</td>\n",
       "      <td>x = 5</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4.88</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A bag has 4 red and 6 blue balls. What is the ...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2/5.</td>\n",
       "      <td>3</td>\n",
       "      <td>4.95</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>If all bloops are meems and some meems are glo...</td>\n",
       "      <td>No, not necessarily</td>\n",
       "      <td>No.</td>\n",
       "      <td>3</td>\n",
       "      <td>6.12</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   k_paths                                           question  \\\n",
       "0        3                                    What is 13 * 7?   \n",
       "1        3  If a train travels 120 km in 2 hours, what is ...   \n",
       "2        3                     Solve for x: 2(x − 3) + 4 = 10   \n",
       "3        3  A bag has 4 red and 6 blue balls. What is the ...   \n",
       "4        3  If all bloops are meems and some meems are glo...   \n",
       "\n",
       "                 truth   predicted  support  latency_sec  correct  \n",
       "0                   91          91        3         4.91     True  \n",
       "1              60 km/h  60 km/hour        3         4.57    False  \n",
       "2                x = 5           6        3         4.88    False  \n",
       "3                  0.4        2/5.        3         4.95    False  \n",
       "4  No, not necessarily         No.        3         6.12    False  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 📊 Inspect detailed results\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✍️ Reflection\n",
    "# In a new markdown cell below, discuss:\n",
    "# - How accuracy changes with k\n",
    "# - Cost/latency implications\n",
    "# - Any qualitative observations about reasoning diversity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reflection\n",
    "Accuracy vs. k.\n",
    "Across self-consistency (majority-vote) runs, accuracy typically increases from k=3 → 5 → 10, but with diminishing returns: most of the gain usually appears by k=5. Occasional non-monotonic dips can happen when answers differ in format (e.g., “40 km/h” vs “40”), so normalizing units/strings before voting helps. The vote margin is a good stability signal—large margins correlate with correctness; narrow margins flag items that merit re-checking.\n",
    "\n",
    "Cost / latency.\n",
    "End-to-end latency and token spend scale roughly linearly with k, since we sample more paths. To keep runtime predictable: (1) cap max_output_tokens (CoT can bloat), (2) prefer concise CoT instructions (“few steps, then Answer:”), (3) tune temperature (lower for arithmetic, higher for logic puzzles), and (4) consider an early-exit heuristic (stop once a candidate surpasses a vote threshold). Handling API timeouts with retries/backoff, plus modest per-request spacing, reduces flaky 504s.\n",
    "\n",
    "Reasoning diversity (qualitative).\n",
    "Higher k surfaces multiple solution modes (algebraic derivation, numeric simulation, analogy/sets). This diversity is valuable: when one mode fails (e.g., arithmetic slip), another often succeeds. Failure modes seen: (a) format drift (units/phrasing), (b) over-reasoning that loses the question, and (c) confident but wrong majority (especially on tricky probability). Mitigations: add lightweight answer-type validators (numeric ranges, unit checks), normalize strings pre-vote, and use a critic/rule-check pass on close votes or high-stakes items. Overall, CoT improves transparency and often accuracy, while self-consistency tames variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Cross‑Model Self‑Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detailed Steps\n",
    "\n",
    "This experiment ensembles **five distinct language models** by majority voting over *one* reasoning path from each model.\n",
    "\n",
    "| Step | What you do | Why it matters |\n",
    "|------|-------------|----------------|\n",
    "| **1. Pick five models** | Populate the `MODELS` list below with at least five different LLM identifiers available to you (e.g., `gpt‑4o`, `claude‑3‑opus`, `gemini‑1.5‑pro‑latest`, `llama‑3‑70b‑instruct`, `mistral‑large`). | Horizontal diversity often yields complementary reasoning. |\n",
    "| **2. Re‑use the tasks** | Use the same `TASKS` list created in Section 6.1 so results are comparable. | Controls for task variation. |\n",
    "| **3. Generate one path per model** | Implement `generate_one_path(question, model)` that returns a single Chain‑of‑Thought response from the given model (*temperature ≈ 0* for deterministic decoding). | Mimics a cost‑constrained ensemble where each model fires once. |\n",
    "| **4. Parse answers** | Re‑use `extract_final_answer` from 6.1 to extract each model’s answer string. | Enables voting. |\n",
    "| **5. Majority vote across models** | Fill in `majority_vote(answers)` (already defined) to aggregate answers across models and return the winning answer + support. | Core of ensemble self‑consistency. |\n",
    "| **6. Run the experiment** | For every task, call each model → vote → record accuracy, per‑task support distribution, latency, and token cost. | Produces cross‑model performance metrics. |\n",
    "| **7. Compare strategies** | Tabulate accuracy/latency/cost vs. the 10‑path single‑model result from Section 6.1. | Quantifies trade‑offs between vertical and horizontal ensembles. |\n",
    "| **8. Reflect** | Discuss which method you’d choose under (a) limited budget, (b) need for highest accuracy, and why. | Connects empirical evidence to deployment choices. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀 Configuration for cross‑model ensemble\n",
    "import time, collections, statistics\n",
    "import pandas as pd\n",
    "\n",
    "MODELS = [\n",
    "    # \"gpt-4o-mini\",\n",
    "    # \"claude-3-haiku\",\n",
    "    \"gemini-1.5-pro\",\n",
    "    # \"llama-3-70b-instruct\",\n",
    "    # \"mistral-large\",\n",
    "]\n",
    "TEMPERATURE_CROSS = 0.0  # Near-deterministic decoding\n",
    "MAX_TOKENS = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_one_path(question: str, model: str) -> str:\n",
    "    \"\"\"Return a Chain‑of‑Thought response from *one* model run.\"\"\"\n",
    "    # TODO: 🔄 Replace the pseudo‑call with your provider's SDK.\n",
    "    # Example:\n",
    "    # response = openai.ChatCompletion.create(\n",
    "    #     model=model,\n",
    "    #     messages=[\n",
    "    #         {\"role\": \"user\", \"content\": f\"{question}\\n\\nLet's think step by step.\"}\n",
    "    #     ],\n",
    "    #     temperature=TEMPERATURE_CROSS,\n",
    "    #     max_tokens=MAX_TOKENS,\n",
    "    # )\n",
    "    # return response['choices'][0]['message']['content']\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "records_cm = []\n",
    "\n",
    "for task in TASKS:\n",
    "    question, truth = task['question'], task['answer']\n",
    "    answers_by_model = {}\n",
    "    t0 = time.time()\n",
    "    for model in MODELS:\n",
    "        resp = generate_one_path(question, model)\n",
    "        ans = extract_final_answer(resp)\n",
    "        answers_by_model[model] = ans\n",
    "    latency = time.time() - t0\n",
    "\n",
    "    voted_answer, support, support_dict = majority_vote(list(answers_by_model.values()))\n",
    "    is_correct = (voted_answer == truth)\n",
    "\n",
    "    records_cm.append({\n",
    "        'question': question,\n",
    "        'truth': truth,\n",
    "        'predicted': voted_answer,\n",
    "        'support': support,\n",
    "        'support_breakdown': support_dict,\n",
    "        'latency_sec': round(latency, 2),\n",
    "        'correct': is_correct,\n",
    "        # TODO: aggregate token usage if your SDK provides it\n",
    "    })\n",
    "\n",
    "df_cm = pd.DataFrame(records_cm)\n",
    "df_cm['correct'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Cross‑model ensemble results\n",
    "df_cm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✍️ Reflection\n",
    "# In a new markdown cell below, compare:\n",
    "# - Accuracy vs. Section 6.1 (k=10)\n",
    "# - Latency & token costs\n",
    "# - Qualitative differences in reasoning styles across models\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
