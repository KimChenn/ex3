{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e7c55dc",
   "metadata": {},
   "source": [
    "# Assignmentâ€¯04 â€“ Reasoning\n",
    "In this assignment you will explore **reasoning** in large language models, experiment with *chainâ€‘ofâ€‘thought* prompting, and reflect on two recent position papers debating whether LLMs truly â€œthink.â€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1&nbsp;Â WhatÂ isÂ Reasoning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f47e6bd",
   "metadata": {},
   "source": [
    "Write **your own definition** of reasoning in the context of intelligent systems.  \n",
    "*Hints:* consider notions such as logical inference, causal deduction, abstraction, multiâ€‘step planning, and how humans articulate intermediate thoughts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2&nbsp;Â BuildÂ aÂ Basicâ€¯Chainâ€‘ofâ€‘ThoughtÂ (CoT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb5f359",
   "metadata": {},
   "source": [
    "### 2â€¯Â·â€¯Buildâ€¯aâ€¯Basicâ€¯Chainâ€‘ofâ€‘Thought (CoT)\n",
    "\n",
    "Your goal is to wrap **any LLM backend of your choice** with a helper that can optionally trigger a *chainâ€‘ofâ€‘thought* style response.\n",
    "\n",
    "#### What to implement\n",
    "\n",
    "1. **Choose a backend** (set `USE_BACKEND`):  \n",
    "   * `\"gemma\"` â€“ use the `google/gemma-3-4b-it` checkpoint via ğŸ¤—Â Transformers.  \n",
    "   * `\"openai\"` â€“ route to yourÂ `call_openai()` helper (e.g., GPTâ€‘4o).  \n",
    "   * `\"gemini\"` â€“ route to yourÂ `call_gemini()` helper (e.g., GeminiÂ 1.5Â Pro).\n",
    "\n",
    "2. **Load / authenticate**  \n",
    "   * HF backends need `HF_TOKEN`.  \n",
    "   * OpenAI backends need `OPENAI_API_KEY`.  \n",
    "   * Gemini backends need `GOOGLE_API_KEY` + `GOOGLE_CSE_ID` or equivalent.\n",
    "\n",
    "3. **Implement `run_llm(prompt, with_cot=False)`**  \n",
    "   * When `with_cot=True`, prepend a CoT trigger such as **â€œLetâ€™s think stepÂ byÂ step.â€**  \n",
    "   * Return the *modelâ€™s final answer* (you may choose to strip the intermediate thoughts).\n",
    "\n",
    "4. **Quick sanityâ€‘check**  \n",
    "   * Call the helper once *without* and once *with* CoT on a simple prompt (e.g., *â€œIs 17 a prime number?â€*) and print both outputs.\n",
    "\n",
    "**Reference:** Jason Wei, Xuezhi Wang, Dale Schuurmans, etâ€¯al.  \n",
    "*â€œChainâ€‘ofâ€‘Thought Prompting Elicits Reasoning in Large Language Models.â€*  \n",
    "arXiv:2201.11903Â (2022) <https://arxiv.org/abs/2201.11903>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def call_openai(prompt: str, model_name: str = \"gpt-4o\") -> str:\n",
    "    \"\"\"Call OpenAI API and return generated text.\"\"\"\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def call_gemini(prompt: str, model_name: str = \"models/gemini-1.5-pro\") -> str:\n",
    "    \"\"\"Call Google Gemini API and return generated text.\"\"\"\n",
    "    import google.generativeai as genai\n",
    "    genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "    model = genai.GenerativeModel(model_name)\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c80f743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- STUDENT TODOs BELOW -----------------------------\n",
    "# 1ï¸âƒ£  Choose your backend: 'gemma', 'openai', or 'gemini'\n",
    "USE_BACKEND = \"gemini\"  # <-- change me\n",
    "\n",
    "# 2ï¸âƒ£  Load / configure the model for the chosen backend\n",
    "if USE_BACKEND == \"gemma\":\n",
    "    # Gemma 3â€‘4B instructionâ€‘tuned via Hugging Face ğŸ§©\n",
    "    from transformers import AutoTokenizer, Gemma3ForConditionalGeneration, pipeline\n",
    "\n",
    "    MODEL_ID = \"google/gemma-3-4b-it\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=True)\n",
    "    model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=\"auto\",\n",
    "        token=True,                # â† relies on HF_TOKEN env variable\n",
    "    )\n",
    "    generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "elif USE_BACKEND == \"openai\":\n",
    "    # OpenAI helper must be defined elsewhere in the notebook\n",
    "    # Example: from my_helpers import call_openai\n",
    "    pass  # TODO: nothing to load â€“ just make sure call_openai() is available\n",
    "\n",
    "elif USE_BACKEND == \"gemini\":\n",
    "    # Gemini helper must be defined elsewhere in the notebook\n",
    "    # Example: from my_helpers import call_gemini\n",
    "    pass  # TODO: nothing to load â€“ just make sure call_gemini() is available\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Unsupported backend selected!\")\n",
    "\n",
    "# 3ï¸âƒ£  Implement the CoT helper\n",
    "def run_llm(prompt: str, *, with_cot: bool = False, max_new_tokens: int = 256):\n",
    "    \"\"\"Run the chosen backend, optionally triggering chainâ€‘ofâ€‘thought.\"\"\"\n",
    "    cot_prefix = \"INSERT YOUR PROMPT HERE\\n\"\n",
    "    full_prompt = cot_prefix + prompt if with_cot else prompt\n",
    "\n",
    "    if USE_BACKEND == \"gemma\":\n",
    "        return generator(full_prompt, max_new_tokens=max_new_tokens)[0][\"generated_text\"]\n",
    "\n",
    "    if USE_BACKEND == \"openai\":\n",
    "        # TODO: ensure `call_openai()` exists\n",
    "        return call_openai(full_prompt)\n",
    "\n",
    "    if USE_BACKEND == \"gemini\":\n",
    "        # TODO: ensure `call_gemini()` exists\n",
    "        return call_gemini(full_prompt)\n",
    "\n",
    "    raise RuntimeError(\"No valid backend route found.\")\n",
    "\n",
    "# 4ï¸âƒ£  Sanityâ€‘check â€“ compare baseline vs. CoT\n",
    "_test_prompt = \"Is 17 a prime number?\"\n",
    "print(\"â†’ Baseline:\", run_llm(_test_prompt, with_cot=False))\n",
    "print(\"â†’ With CoT:\", run_llm(_test_prompt, with_cot=True))\n",
    "# -------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3&nbsp;Â Evaluateâ€¯CoTâ€¯Performance vs. Singleâ€‘Call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ed33e0",
   "metadata": {},
   "source": [
    "\n",
    "Pick **10 reasoningâ€‘intensive questions** (e.g. a logic puzzle, word problem, or multiâ€‘step arithmetic query).  \n",
    "Run the model twice: once *without* chainâ€‘ofâ€‘thought and once *with* CoT using `run_llm`.  \n",
    "Manually (or programmatically) judge which output is *more correct, complete, and transparent*.  \n",
    "Record your observations below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aac7ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â–¶ï¸ Comparison template\n",
    "reasoning_question = \"If all bloops are meems and some meems are glorps, are all bloops definitely glorps? Explain your answer.\"\n",
    "\n",
    "baseline = run_llm(reasoning_question, with_cot=False)\n",
    "cot = run_llm(reasoning_question, with_cot=True)\n",
    "\n",
    "print(\"\\n--- Baseline ---\\n\", baseline)\n",
    "print(\"\\n--- CoT ---\\n\", cot)\n",
    "\n",
    "# TODO: Add your evaluation notes (e.g. accuracy, clarity) in the markdown cell that follows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4&nbsp;Â ReadÂ Two Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3c16ac",
   "metadata": {},
   "source": [
    "\n",
    "* **â€œThe Illusion of Thinking: Understanding the Strengths and Limitations of Reasoningâ€¯Models via the Lens of Problemâ€¯Complexityâ€** (Shojaee *etâ€¯al.*, 2025).  \n",
    "  *ArXiv:* <https://arxiv.org/abs/2506.06941>\n",
    "* **â€œThe Illusion of the Illusion of Thinkingâ€** (Opus & Lawson, 2025) â€“ a critical response.  \n",
    "  *ArXiv:* <https://arxiv.org/abs/2506.09250>\n",
    "\n",
    "Skim both (abstractâ€¯â†’â€¯methodsâ€¯â†’â€¯main results) and take note of their competing claims about LRMs and chainâ€‘ofâ€‘thought reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5&nbsp;Â ReflectionÂ (2â€¯paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02422c52",
   "metadata": {},
   "source": [
    "\n",
    "In **â‰ˆ150â€“250â€¯words**, explain **which paperâ€™s argument you find more convincing andâ€¯why**.  \n",
    "Consider the authorsâ€™ experimental setups, evidence, interpretation of â€œreasoning,â€ and any limitations you notice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## SectionÂ 6Â â€”Â Implementing Selfâ€‘Consistency Decoding\n",
    "\n",
    "**Learning goal.** Experience how selfâ€‘consistency improves reasoning accuracy by sampling diverse chainâ€‘ofâ€‘thoughts and aggregating answers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1Â Singleâ€‘Model Selfâ€‘Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DetailedÂ Steps\n",
    "\n",
    "Follow the eight steps below to implement selfâ€‘consistency with a *single* language model.\n",
    "\n",
    "| Step | What you do | Why it matters |\n",
    "|------|-------------|----------------|\n",
    "| **1. Choose a base model** | Pick **one** of the foundation models you have tried earlier (e.g., GPTâ€‘4o, Claudeâ€‘3Â Sonnet, GeminiÂ 1.5Â Pro, etc.). Set it in the `MODEL` constant below. | Keeps the experiment focused and costâ€‘controlled. |\n",
    "| **2. Select tasks** | Reâ€‘use at least **five reasoning problems** from previous sections and store them in `TASKS`. Each task must include a groundâ€‘truth answer for evaluation. | Ensures comparability across sampling budgets. |\n",
    "| **3. Generate reasoning paths** | Write `generate_paths(question, n_paths)` that samples `n_paths` Chainâ€‘ofâ€‘Thought explanations from the chosen model (temperature â‰¥Â 0.7). | Diversity of reasoning paths is the heart of selfâ€‘consistency. |\n",
    "| **4. Parse final answers** | Implement `extract_final_answer(full_response)` that returns the answer string from a model response. | Needed for voting. |\n",
    "| **5. Majority vote aggregator** | Implement `majority_vote(answers)` that returns the most frequent answer and its support size. Break ties by picking the answer whose chain has the highest average logâ€‘probability. | Converts diverse chains into a single prediction. |\n",
    "| **6. Run experiments** | For each sampling budget **kÂ âˆˆÂ {3,Â 5,Â 10}** and each task, generate `k` paths â†’ vote â†’ record whether the voted answer matches ground truth. | Measures accuracy vs. compute. |\n",
    "| **7. Collect metrics** | Track (a) accuracy, (b) average latency, (c) total tokens. Store them in a `pandas.DataFrame`. | Enables quantitative comparison. |\n",
    "| **8. Analyze results** | Plot / tabulate metrics and write a short discussion: *Where do returns diminish? How does cost scale?* | Connects empirical findings to theory. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ Imports & configuration\n",
    "import time, json, collections, statistics\n",
    "import pandas as pd\n",
    "\n",
    "# TODO: ğŸ”‘ Add your API key if needed, e.g. openai.api_key = \"sk-...\"\n",
    "# import openai\n",
    "\n",
    "MODEL = \"gpt-4o-mini\"  # TODO: replace with your chosen model\n",
    "TEMPERATURE = 0.7      # Nonâ€‘zero for diverse sampling\n",
    "MAX_TOKENS = 1200\n",
    "\n",
    "# ğŸ‘‡ Populate with at least 5 {question, answer} dicts\n",
    "TASKS = [\n",
    "    # {\"question\": \"What is 13 * 7?\", \"answer\": \"91\"},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_paths(question: str, n_paths: int):\n",
    "    \"\"\"Return a list of full model responses (thought + answer).\"\"\"\n",
    "    paths = []\n",
    "    for _ in range(n_paths):\n",
    "        # TODO: ğŸ”„ Call your LLM here with Chainâ€‘ofâ€‘Thought prompting\n",
    "        # Example pseudoâ€‘call (replace with the correct SDK/method):\n",
    "        # response = openai.ChatCompletion.create(\n",
    "        #     model=MODEL,\n",
    "        #     messages=[\n",
    "        #         {\"role\": \"user\", \"content\": f\"{question}\\n\\nLet's think step by step.\"}\n",
    "        #     ],\n",
    "        #     temperature=TEMPERATURE,\n",
    "        #     max_tokens=MAX_TOKENS,\n",
    "        # )\n",
    "        # paths.append(response['choices'][0]['message']['content'])\n",
    "        pass\n",
    "    return paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_answer(response: str) -> str:\n",
    "    \"\"\"Extract the answer after the last occurrence of 'Answer:' (simple heuristic).\"\"\"\n",
    "    import re\n",
    "    match = re.findall(r\"Answer\\s*[:=]\\s*(.*)\", response, flags=re.IGNORECASE)\n",
    "    return match[-1].strip() if match else \"\"\n",
    "\n",
    "def majority_vote(answers):\n",
    "    \"\"\"Return (winning_answer, support_count, counts_dict).\"\"\"\n",
    "    counts = collections.Counter(answers)\n",
    "    winner, support = counts.most_common(1)[0]\n",
    "    return winner, support, counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "records = []\n",
    "\n",
    "for k in [3, 5, 10]:\n",
    "    for task in TASKS:\n",
    "        question, truth = task['question'], task['answer']\n",
    "        t0 = time.time()\n",
    "        paths = generate_paths(question, k)\n",
    "        gen_time = time.time() - t0\n",
    "\n",
    "        answers = [extract_final_answer(p) for p in paths]\n",
    "        pred, support, _ = majority_vote(answers)\n",
    "        is_correct = (pred == truth)\n",
    "\n",
    "        records.append({\n",
    "            'k_paths': k,\n",
    "            'question': question,\n",
    "            'truth': truth,\n",
    "            'predicted': pred,\n",
    "            'support': support,\n",
    "            'latency_sec': round(gen_time, 2),\n",
    "            'correct': is_correct,\n",
    "            # TODO: add token_usage if available from your SDK\n",
    "        })\n",
    "\n",
    "df_results = pd.DataFrame(records)\n",
    "df_results.groupby('k_paths')['correct'].mean().rename('accuracy').to_frame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Inspect detailed results\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœï¸ Reflection\n",
    "# In a new markdown cell below, discuss:\n",
    "# - How accuracy changes with k\n",
    "# - Cost/latency implications\n",
    "# - Any qualitative observations about reasoning diversity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2Â Crossâ€‘Model Selfâ€‘Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DetailedÂ Steps\n",
    "\n",
    "This experiment ensembles **five distinct language models** by majority voting over *one* reasoning path from each model.\n",
    "\n",
    "| Step | What you do | Why it matters |\n",
    "|------|-------------|----------------|\n",
    "| **1. Pick five models** | Populate the `MODELS` list below with at least five different LLM identifiers available to you (e.g., `gptâ€‘4o`, `claudeâ€‘3â€‘opus`, `geminiâ€‘1.5â€‘proâ€‘latest`, `llamaâ€‘3â€‘70bâ€‘instruct`, `mistralâ€‘large`). | Horizontal diversity often yields complementary reasoning. |\n",
    "| **2. Reâ€‘use the tasks** | Use the same `TASKS` list created in SectionÂ 6.1 so results are comparable. | Controls for task variation. |\n",
    "| **3. Generate one path per model** | Implement `generate_one_path(question, model)` that returns a single Chainâ€‘ofâ€‘Thought response from the given model (*temperature â‰ˆÂ 0* for deterministic decoding). | Mimics a costâ€‘constrained ensemble where each model fires once. |\n",
    "| **4. Parse answers** | Reâ€‘use `extract_final_answer` from 6.1 to extract each modelâ€™s answer string. | Enables voting. |\n",
    "| **5. Majority vote across models** | Fill in `majority_vote(answers)` (already defined) to aggregate answers across models and return the winning answer + support. | Core of ensemble selfâ€‘consistency. |\n",
    "| **6. Run the experiment** | For every task, call each model â†’ vote â†’ record accuracy, perâ€‘task support distribution, latency, and token cost. | Produces crossâ€‘model performance metrics. |\n",
    "| **7. Compare strategies** | Tabulate accuracy/latency/cost vs. the 10â€‘path singleâ€‘model result from SectionÂ 6.1. | Quantifies tradeâ€‘offs between vertical and horizontal ensembles. |\n",
    "| **8. Reflect** | Discuss which method youâ€™d choose under (a) limited budget, (b) need for highest accuracy, and why. | Connects empirical evidence to deployment choices. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ Configuration for crossâ€‘model ensemble\n",
    "import time, collections, statistics\n",
    "import pandas as pd\n",
    "\n",
    "MODELS = [\n",
    "    # \"gpt-4o-mini\",\n",
    "    # \"claude-3-haiku\",\n",
    "    # \"gemini-1.5-pro\",\n",
    "    # \"llama-3-70b-instruct\",\n",
    "    # \"mistral-large\",\n",
    "]\n",
    "TEMPERATURE_CROSS = 0.0  # Near-deterministic decoding\n",
    "MAX_TOKENS = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_one_path(question: str, model: str) -> str:\n",
    "    \"\"\"Return a Chainâ€‘ofâ€‘Thought response from *one* model run.\"\"\"\n",
    "    # TODO: ğŸ”„ Replace the pseudoâ€‘call with your provider's SDK.\n",
    "    # Example:\n",
    "    # response = openai.ChatCompletion.create(\n",
    "    #     model=model,\n",
    "    #     messages=[\n",
    "    #         {\"role\": \"user\", \"content\": f\"{question}\\n\\nLet's think step by step.\"}\n",
    "    #     ],\n",
    "    #     temperature=TEMPERATURE_CROSS,\n",
    "    #     max_tokens=MAX_TOKENS,\n",
    "    # )\n",
    "    # return response['choices'][0]['message']['content']\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "records_cm = []\n",
    "\n",
    "for task in TASKS:\n",
    "    question, truth = task['question'], task['answer']\n",
    "    answers_by_model = {}\n",
    "    t0 = time.time()\n",
    "    for model in MODELS:\n",
    "        resp = generate_one_path(question, model)\n",
    "        ans = extract_final_answer(resp)\n",
    "        answers_by_model[model] = ans\n",
    "    latency = time.time() - t0\n",
    "\n",
    "    voted_answer, support, support_dict = majority_vote(list(answers_by_model.values()))\n",
    "    is_correct = (voted_answer == truth)\n",
    "\n",
    "    records_cm.append({\n",
    "        'question': question,\n",
    "        'truth': truth,\n",
    "        'predicted': voted_answer,\n",
    "        'support': support,\n",
    "        'support_breakdown': support_dict,\n",
    "        'latency_sec': round(latency, 2),\n",
    "        'correct': is_correct,\n",
    "        # TODO: aggregate token usage if your SDK provides it\n",
    "    })\n",
    "\n",
    "df_cm = pd.DataFrame(records_cm)\n",
    "df_cm['correct'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Crossâ€‘model ensemble results\n",
    "df_cm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœï¸ Reflection\n",
    "# In a new markdown cell below, compare:\n",
    "# - Accuracy vs. SectionÂ 6.1 (k=10)\n",
    "# - Latency & token costs\n",
    "# - Qualitative differences in reasoning styles across models\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
